{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GetPrediction_From_Checkpoint_deepGraphh.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Make a copy of this python notebook**"
      ],
      "metadata": {
        "id": "no905oYzbFFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Installation"
      ],
      "metadata": {
        "id": "3GorFGOeEJS6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34a790MDx70K",
        "outputId": "89e663f2-991f-4532-df23-a7804189256a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3457  100  3457    0     0   8909      0 --:--:-- --:--:-- --:--:--  8909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "add /root/miniconda/lib/python3.7/site-packages to PYTHONPATH\n",
            "python version: 3.7.12\n",
            "fetching installer from https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "done\n",
            "installing miniconda to /root/miniconda\n",
            "done\n",
            "installing openmm, pdbfixer\n",
            "added conda-forge to channels\n",
            "done\n",
            "conda packages installation finished!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# conda environments:\n",
            "#\n",
            "base                  *  /root/miniconda\n",
            "\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.6.0.dev20211215231347-py3-none-any.whl (608 kB)\n",
            "\u001b[K     |████████████████████████████████| 608 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.1.5)\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2021.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.6 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->deepchem) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deepchem) (3.0.0)\n",
            "Installing collected packages: rdkit-pypi, deepchem\n",
            "Successfully installed deepchem-2.6.0.dev20211215231347 rdkit-pypi-2021.9.3\n"
          ]
        }
      ],
      "source": [
        "!curl -Lo conda_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "import conda_installer\n",
        "conda_installer.install()\n",
        "!/root/miniconda/bin/conda info -e\n",
        "!pip install --pre deepchem\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dgl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvzn4SfZym1X",
        "outputId": "abd7a932-8a17-4cb8-cd20-9b02dbaee923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dgllife"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVAgJGtTzlF4",
        "outputId": "33bdaf4d-967e-4384-deaa-18d0edb6284d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgllife\n",
            "  Downloading dgllife-0.2.8.tar.gz (133 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 61 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 92 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from dgllife) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgllife) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgllife) (2.6.3)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (from dgllife) (0.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->dgllife) (3.0.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt->dgllife) (3.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt->dgllife) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt->dgllife) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->dgllife) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->dgllife) (2018.9)\n",
            "Building wheels for collected packages: dgllife\n",
            "  Building wheel for dgllife (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dgllife: filename=dgllife-0.2.8-py3-none-any.whl size=213162 sha256=7f681e5f35332637144cd36752b903ce59c3ef86d691ace1c02b358a74bef465\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/81/6f/6bbc4b7a80b06d92829b5362a78277173c9e7b46d294d4ca45\n",
            "Successfully built dgllife\n",
            "Installing collected packages: dgllife\n",
            "Successfully installed dgllife-0.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import deepchem as dc\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from deepchem.models import GCNModel\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from itertools import cycle\n",
        "from scipy import interp\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "# import init_db as database\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xiPbwN1OykAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount your drive"
      ],
      "metadata": {
        "id": "_N_Qa1MpaEsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KUW_Z_TMaD0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Information"
      ],
      "metadata": {
        "id": "4sRRYf0FEPUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter the model name in mode section\n",
        "mode = 'Select_Model' # mode ={\"gcn\": Graph Convolution Model,\"gat\": Graph Attentive Network ,\"attentive\": Attentive FP,\"dag\": Directed Acyclic Graph}\n",
        "\n",
        "# Enter the path of query data\n",
        "data_path = 'Path of the the query data'\n",
        "\n",
        "# Enter the path of the directory where the model checkpoint is saved\n",
        "path_checkpoint = 'Directory path of the checkpoint'"
      ],
      "metadata": {
        "id": "e1l1zF_A3YL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deepGraphh"
      ],
      "metadata": {
        "id": "n7drJgtwEl5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this section to get the results for query data\n",
        "##*************** Please do not make any changes in this section****************##\n",
        "\n",
        "data = pd.read_csv(data_path )\n",
        "\n",
        "if mode == 'gcn':\n",
        "  featurizer = dc.feat.MolGraphConvFeaturizer()\n",
        "  data = data[['SMILES', 'Activation Status']]\n",
        "  ret = featurizer.featurize(data['SMILES'])\n",
        "  indexes_notF = list()\n",
        "  for i in range(0, len(data)):\n",
        "    if not ret[i]:\n",
        "      indexes_notF.append(i)\n",
        "  data = data.drop(indexes_notF)\n",
        "  with dc.utils.UniversalNamedTemporaryFile(mode='w') as tmpfile:\n",
        "    data.to_csv(tmpfile.name)\n",
        "    loader = dc.data.CSVLoader([\"Activation Status\"], feature_field=\"SMILES\",\n",
        "                                featurizer=dc.feat.MolGraphConvFeaturizer())\n",
        "    dataset = loader.create_dataset(tmpfile.name)\n",
        "  model = dc.models.GCNModel(mode='classification', n_tasks=1, batch_size=32, learning_rate=0.001,n_classes=2,model_dir=path_checkpoint)\n",
        "\n",
        "  model.restore(model_dir=path_checkpoint)\n",
        "  user_predict_data = model.predict(dataset)\n",
        "  pred = np.array(user_predict_data)\n",
        "  for i in range(0, len(pred[0])):\n",
        "    ithcol = pred[:, i]\n",
        "    stringClass = 'class ' + str(i)\n",
        "    data[stringClass] = ithcol\n",
        "  data.to_csv('Input_data_Prob_Matrix.csv')\n",
        "\n",
        "\n",
        "if mode == 'gat':\n",
        "    featurizer = dc.feat.MolGraphConvFeaturizer()\n",
        "    data = data[['SMILES', 'Activation Status']]\n",
        "    ret = featurizer.featurize(data['SMILES'])\n",
        "    indexes_notF = list()\n",
        "    for i in range(0, len(data)):\n",
        "      if not ret[i]:\n",
        "        indexes_notF.append(i)\n",
        "    data = data.drop(indexes_notF)\n",
        "    with dc.utils.UniversalNamedTemporaryFile(mode='w') as tmpfile:\n",
        "      data.to_csv(tmpfile.name)\n",
        "      loader = dc.data.CSVLoader([\"Activation Status\"], feature_field=\"SMILES\",\n",
        "                                 featurizer=dc.feat.MolGraphConvFeaturizer())\n",
        "      dataset = loader.create_dataset(tmpfile.name)\n",
        "    \n",
        "    model = dc.models.GATModel(mode='classification', n_tasks=1, batch_size=32, learning_rate=0.001,n_classes = 2,model_dir=path_checkpoint)\n",
        "\n",
        "    model.restore(model_dir=path_checkpoint)\n",
        "    user_predict_data = model.predict(dataset)\n",
        "    pred = np.array(user_predict_data)\n",
        "    for i in range(0, len(pred[0])):\n",
        "      ithcol = pred[:, i]\n",
        "      stringClass = 'class ' + str(i)\n",
        "      data[stringClass] = ithcol\n",
        "    data.to_csv('Input_data_Prob_Matrix.csv')\n",
        "\n",
        "if mode == 'attentive':\n",
        "  featurizer = dc.feat.MolGraphConvFeaturizer()\n",
        "  data = data[['SMILES', 'Activation Status']]\n",
        "  ret = featurizer.featurize(data['SMILES'])\n",
        "  indexes_notF = list()\n",
        "  for i in range(0, len(data)):\n",
        "    if not ret[i]:\n",
        "      indexes_notF.append(i)\n",
        "  data = data.drop(indexes_notF)\n",
        "  with dc.utils.UniversalNamedTemporaryFile(mode='w') as tmpfile:\n",
        "    data.to_csv(tmpfile.name)\n",
        "    loader = dc.data.CSVLoader([\"Activation Status\"], feature_field=\"SMILES\",\n",
        "                                featurizer=dc.feat.MolGraphConvFeaturizer(use_edges=True))\n",
        "    dataset = loader.create_dataset(tmpfile.name)\n",
        "  \n",
        "  model = dc.models.AttentiveFPModel(mode='classification', n_tasks=1, batch_size=32, learning_rate=0.001,n_classes = 2,model_dir=path_checkpoint)\n",
        "\n",
        "  model.restore(model_dir=path_checkpoint)\n",
        "  user_predict_data = model.predict(dataset)\n",
        "  pred = np.array(user_predict_data)\n",
        "  for i in range(0, len(pred[0])):\n",
        "    ithcol = pred[:, i]\n",
        "    stringClass = 'class ' + str(i)\n",
        "    data[stringClass] = ithcol\n",
        "  data.to_csv('Input_data_Prob_Matrix.csv')\n",
        "\n",
        "\n",
        "if mode == 'dag':\n",
        "  featurizer = dc.feat.ConvMolFeaturizer()\n",
        "  data = data[['SMILES', 'Activation Status']]\n",
        "  ret = featurizer.featurize(data['SMILES'])\n",
        "  indexes_notF = list()\n",
        "  for i in range(0, len(data)):\n",
        "    if not ret[i]:\n",
        "      indexes_notF.append(i)\n",
        "  data = data.drop(indexes_notF)\n",
        "  with dc.utils.UniversalNamedTemporaryFile(mode='w') as tmpfile:\n",
        "    data.to_csv(tmpfile.name)\n",
        "    loader = dc.data.CSVLoader([\"Activation Status\"], feature_field=\"SMILES\",\n",
        "                                featurizer=dc.feat.ConvMolFeaturizer())\n",
        "    dataset = loader.create_dataset(tmpfile.name)\n",
        "  trans = dc.trans.DAGTransformer(max_atoms=50)\n",
        "  dataset = trans.transform(dataset)\n",
        "\n",
        "  model = dc.models.DAGModel(mode='classification', n_tasks=1, batch_size=32, learning_rate=0.001,n_classes = 2,model_dir=path_checkpoint)\n",
        "\n",
        "  model.restore(model_dir=path_checkpoint)\n",
        "  user_predict_data = model.predict(dataset)\n",
        "  pred = np.array(user_predict_data)\n",
        "  for i in range(0, len(pred[0][0])):\n",
        "    ithcol = list()\n",
        "    for k in range(0, len(pred)):\n",
        "      ithcol.append(pred[k][0][i])\n",
        "    stringClass = 'class ' + str(i)\n",
        "    data[stringClass] = ithcol\n",
        "  data.to_csv('Input_data_Prob_Matrix.csv')\n",
        "  ## Result will be saved in the same folder as path_checkpoint, user can change the path if necessary."
      ],
      "metadata": {
        "id": "0q-OWgB21r_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xcR13BtN3Kde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}